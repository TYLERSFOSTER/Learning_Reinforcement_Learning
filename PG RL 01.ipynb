{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7d75a1fb-2e12-4bbd-a4f3-d1fa4516a9c7",
   "metadata": {},
   "source": [
    "# Policy Gradient RL model 001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "00ace8e9-3294-4f1b-aebe-92e837f82757",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Torch version: 1.11.0\n"
     ]
    }
   ],
   "source": [
    "import copy\n",
    "\n",
    "import random\n",
    "import math\n",
    "import numpy as np\n",
    "from inspect import isfunction\n",
    "\n",
    "import torch\n",
    "print('Torch version:', torch.__version__)\n",
    "\n",
    "import torch.nn as nn\n",
    "import torch.nn.init as init\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "import sys"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d147c1f6-cb24-4c7f-ac85-3d2b44ade7a5",
   "metadata": {},
   "source": [
    "## Policy and agent classes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "id": "34635bb8-89aa-4bda-ba87-ca36ca403a33",
   "metadata": {},
   "outputs": [],
   "source": [
    "state_space = [i for i in range(0, 128)]\n",
    "state_space_tensor = torch.Tensor([float(i) for i in range(0, 128)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "id": "48ef93a2-dd6c-4ef8-bf51-921e41ff2da2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([  0.,   1.,   2.,   3.,   4.,   5.,   6.,   7.,   8.,   9.,  10.,  11.,\n",
      "         12.,  13.,  14.,  15.,  16.,  17.,  18.,  19.,  20.,  21.,  22.,  23.,\n",
      "         24.,  25.,  26.,  27.,  28.,  29.,  30.,  31.,  32.,  33.,  34.,  35.,\n",
      "         36.,  37.,  38.,  39.,  40.,  41.,  42.,  43.,  44.,  45.,  46.,  47.,\n",
      "         48.,  49.,  50.,  51.,  52.,  53.,  54.,  55.,  56.,  57.,  58.,  59.,\n",
      "         60.,  61.,  62.,  63.,  64.,  65.,  66.,  67.,  68.,  69.,  70.,  71.,\n",
      "         72.,  73.,  74.,  75.,  76.,  77.,  78.,  79.,  80.,  81.,  82.,  83.,\n",
      "         84.,  85.,  86.,  87.,  88.,  89.,  90.,  91.,  92.,  93.,  94.,  95.,\n",
      "         96.,  97.,  98.,  99., 100., 101., 102., 103., 104., 105., 106., 107.,\n",
      "        108., 109., 110., 111., 112., 113., 114., 115., 116., 117., 118., 119.,\n",
      "        120., 121., 122., 123., 124., 125., 126., 127.])\n"
     ]
    }
   ],
   "source": [
    "print(state_space_tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "id": "862077e9-916f-4588-8dd9-a8864218169a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Policy(nn.Module):\n",
    "    def __init__(self, input_features=128, out_features=128, layer_count=4, hidden_features=600, dropout=.1):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.input_features = input_features\n",
    "        self.out_features = out_features\n",
    "        self.layer_count = layer_count\n",
    "        self.hidden_features = hidden_features\n",
    "        self.dropout = dropout\n",
    "        \n",
    "        self.layers = nn.ModuleList()\n",
    "        self.layers.append(nn.Linear(2*self.input_features, self.hidden_features))\n",
    "        self.layers.append(torch.nn.Dropout(p=self.dropout))\n",
    "        #self.layers.append(nn.BatchNorm1d(self.hidden_features))\n",
    "        self.layers.append(nn.LeakyReLU())\n",
    "        for n in range(self.layer_count-2):\n",
    "            self.layers.append(nn.Linear(self.hidden_features, self.hidden_features))\n",
    "            self.layers.append(torch.nn.Dropout(p=self.dropout))\n",
    "            #self.layers.append(nn.BatchNorm1d(self.hidden_features))\n",
    "            self.layers.append(nn.LeakyReLU())\n",
    "        self.layers.append(nn.Linear(self.hidden_features, self.out_features))\n",
    "        self.layers.append(nn.Softmax(dim=-1))\n",
    "        \n",
    "    def forward(self, goal_dist, state_dist):\n",
    "        x = torch.cat((state_dist, goal_dist))\n",
    "        for layer in self.layers:\n",
    "            x = layer(x)\n",
    "        output_dist = x\n",
    "        action_index = torch.argmax(x).unsqueeze(-1)\n",
    "        return output_dist, action_index"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e38d03c4-1209-4327-8be8-16693c208063",
   "metadata": {},
   "source": [
    "**Testing:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "id": "4b059288-7194-4cb4-9bc5-eb62d686de0f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Layer list:\n",
      " ModuleList(\n",
      "  (0): Linear(in_features=256, out_features=600, bias=True)\n",
      "  (1): Dropout(p=0.1, inplace=False)\n",
      "  (2): LeakyReLU(negative_slope=0.01)\n",
      "  (3): Linear(in_features=600, out_features=600, bias=True)\n",
      "  (4): Dropout(p=0.1, inplace=False)\n",
      "  (5): LeakyReLU(negative_slope=0.01)\n",
      "  (6): Linear(in_features=600, out_features=600, bias=True)\n",
      "  (7): Dropout(p=0.1, inplace=False)\n",
      "  (8): LeakyReLU(negative_slope=0.01)\n",
      "  (9): Linear(in_features=600, out_features=128, bias=True)\n",
      "  (10): Softmax(dim=-1)\n",
      ")\n",
      "\n",
      "Length of output distribution: 128\n",
      "\n",
      "Sum of output distribution: tensor(1.0000, grad_fn=<SumBackward0>)\n",
      "\n",
      "Policy's action: tensor([91])\n"
     ]
    }
   ],
   "source": [
    "policy = Policy()\n",
    "print('\\nLayer list:\\n', policy.layers)\n",
    "\n",
    "goal_dist = torch.rand(128)\n",
    "state_dist = torch.rand(128)\n",
    "print('\\nLength of output distribution:', len(policy(state_dist, goal_dist)[0]))\n",
    "print('\\nSum of output distribution:', policy(state_dist, goal_dist)[0].sum())\n",
    "print('\\nPolicy\\'s action:', policy(state_dist, goal_dist)[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 265,
   "id": "30ad4489-57fc-4269-a0c9-9415dabb32aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Agent():\n",
    "    def __init__(self, policy=Policy(), goal_reward = 20, epsilon=.2):\n",
    "        \n",
    "        self.policy = policy\n",
    "        self.goal_reward = torch.Tensor([goal_reward])\n",
    "        \n",
    "        self.epsilon = epsilon\n",
    "        \n",
    "        self.episode_states = torch.Tensor()\n",
    "        self.episode_rewards = torch.Tensor()\n",
    "        \n",
    "        self.episode_goal = torch.Tensor()\n",
    "        \n",
    "        self.state_space_tensor = torch.Tensor([float(i) for i in range(0, 128)])\n",
    "        \n",
    "        self.explore_possibilities = ['explore', 'on_policy']\n",
    "        self.explore_weights = [self.epsilon, 1.-self.epsilon]\n",
    "        \n",
    "    def reward(self, goal_index, state_index, action_index):\n",
    "        action_direction = torch.sign(action_index - state_index)\n",
    "        goal_direction = torch.sign(goal_index - state_index)\n",
    "        \n",
    "        direction_reward = action_direction * goal_direction\n",
    "        \n",
    "        goal_reward = self.goal_reward * torch.eq(action_index, goal_index)\n",
    "        \n",
    "        return direction_reward + goal_reward\n",
    "        \n",
    "    def initiate_episode(self, goal_index, start_index):\n",
    "        self.episode_goal = torch.Tensor([goal_index])\n",
    "        \n",
    "        self.episode_states = torch.Tensor([start_index])\n",
    "        self.episode_rewards = torch.Tensor([0.])\n",
    "        \n",
    "    def act(self):\n",
    "        last_state_index = self.episode_states[-1]\n",
    "        last_state_dist = torch.eq(self.state_space_tensor, last_state_index).float()\n",
    "    \n",
    "        goal_dist = torch.eq(self.state_space_tensor, self.episode_goal).float()\n",
    "        \n",
    "        explore_Q = random.choices(self.explore_possibilities, self.explore_weights)\n",
    "        if explore_Q == self.explore_possibilities[0]:\n",
    "            new_action_index = torch.Tensor([random.randrange(0, 128)])\n",
    "        else:\n",
    "            new_action_index = policy(goal_dist, last_state_dist)[1]\n",
    "            \n",
    "        self.episode_states = torch.cat((self.episode_states, new_action_index))\n",
    "            \n",
    "        new_reward = self.reward(self.episode_goal, last_state_index, new_action_index)\n",
    "            \n",
    "        self.episode_rewards = torch.cat((self.episode_rewards, new_reward))\n",
    "        \n",
    "    def generate_episode(self, goal_index, start_index, time_step_count):\n",
    "        self.initiate_episode(goal_index, start_index)\n",
    "        step_index = 0\n",
    "        while (step_index < time_step_count) & (self.episode_states[-1] != self.episode_goal):\n",
    "            self.act()\n",
    "            step_index += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd7d76bc-a2f8-4239-a9fd-9beb6d05ef04",
   "metadata": {},
   "source": [
    "**Testing:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 264,
   "id": "eb4b2584-a721-42b2-a190-4bbbae3e5d99",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Un-nitialized episode state list: tensor([])\n",
      "Un-nitialized episode reward list: tensor([])\n",
      "\n",
      "Generic reward: tensor([1.])\n",
      "\n",
      "Reward in case where action takes us to goal: tensor([21.])\n",
      "\n",
      "Reward in case where state = action = goal: tensor([20.])\n",
      "\n",
      "___________________________________________________\n",
      "\n",
      "\n",
      "Initialized episode goal: tensor([56.])\n",
      "Initialized episode state list: tensor([90.])\n",
      "Initialized episode reward list: tensor([0.])\n",
      "\n",
      "Episode state list after one action: tensor([ 90., 110.])\n",
      "Episode reward list after one action: tensor([ 0., -1.])\n",
      "\n",
      "Episode state list after two actions: tensor([ 90., 110.,  91.])\n",
      "Episode reward list after two actions: tensor([ 0., -1.,  1.])\n",
      "\n",
      "___________________________________________________\n",
      "\n",
      "\n",
      "Initialized episode goal: tensor([56.])\n",
      "Initialized episode state list: tensor([90.])\n",
      "Initialized episode reward list: tensor([0.])\n",
      "\n",
      "Episode state list after 30 actions or goal:\n",
      " tensor([ 90.,  91.,  91.,  94.,  91.,  91.,  91.,  91., 110.,  91.,  53.,  91.,\n",
      "         91.,  91.,  56.])\n",
      "Episode reward list after 30 actions or goal:\n",
      " tensor([ 0., -1.,  0., -1.,  1.,  0.,  0.,  0., -1.,  1.,  1.,  1.,  0.,  0.,\n",
      "        21.])\n"
     ]
    }
   ],
   "source": [
    "agent = Agent()\n",
    "#print('\\nAgent\\'s policy model:\\n', agent.policy)\n",
    "\n",
    "print('\\nUn-nitialized episode state list:', agent.episode_states)\n",
    "print('Un-nitialized episode reward list:', agent.episode_rewards)\n",
    "\n",
    "goal_index = torch.Tensor([random.randrange(0,128)])\n",
    "state_index = torch.Tensor([random.randrange(0,128)])\n",
    "action_index = torch.Tensor([random.randrange(0,128)])\n",
    "print('\\nGeneric reward:', agent.reward(goal_index, state_index, action_index))\n",
    "print('\\nReward in case where action takes us to goal:', agent.reward(goal_index, state_index, goal_index))\n",
    "print('\\nReward in case where state = action = goal:', agent.reward(goal_index, goal_index, goal_index))\n",
    "\n",
    "print('\\n___________________________________________________\\n')\n",
    "\n",
    "agent.initiate_episode(goal_index.item(), state_index.item())\n",
    "print('\\nInitialized episode goal:', agent.episode_goal)\n",
    "print('Initialized episode state list:', agent.episode_states)\n",
    "print('Initialized episode reward list:', agent.episode_rewards)\n",
    "\n",
    "agent.act()\n",
    "print('\\nEpisode state list after one action:', agent.episode_states)\n",
    "print('Episode reward list after one action:', agent.episode_rewards)\n",
    "\n",
    "agent.act()\n",
    "print('\\nEpisode state list after two actions:', agent.episode_states)\n",
    "print('Episode reward list after two actions:', agent.episode_rewards)\n",
    "\n",
    "print('\\n___________________________________________________\\n')\n",
    "\n",
    "agent.initiate_episode(goal_index.item(), state_index.item())\n",
    "print('\\nInitialized episode goal:', agent.episode_goal)\n",
    "print('Initialized episode state list:', agent.episode_states)\n",
    "print('Initialized episode reward list:', agent.episode_rewards)\n",
    "\n",
    "step_count = 30\n",
    "agent.generate_episode(goal_index.item(), state_index.item(), step_count)\n",
    "print('\\nEpisode state list after {} actions or goal:\\n'.format(step_count), agent.episode_states)\n",
    "print('Episode reward list after {} actions or goal:\\n'.format(step_count), agent.episode_rewards)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "114c5d79-e09f-4d3b-963c-2626aba0cf43",
   "metadata": {},
   "source": [
    "## Training loop."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6417c702-9dcf-4cc2-abb1-55dc3b544cac",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01615639-d08c-4bfe-8403-64aa4f6b203c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

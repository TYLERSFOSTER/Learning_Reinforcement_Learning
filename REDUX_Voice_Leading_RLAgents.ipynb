{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "54a1e37e",
   "metadata": {},
   "source": [
    "# Voice leading reinforcement learning agents."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77219ad5",
   "metadata": {},
   "source": [
    "## Introduction.\n",
    "[...]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45011e38",
   "metadata": {},
   "source": [
    "$$\n",
    "\\mathbb{E}[\\text{return}|\\alpha, s]\n",
    "\\ =\\ \n",
    "\\text{reward}+\\underset{\\ \\beta\\ \\in\\ \\mathcal{A}_{\\alpha(s)}\\!\\!}{\\text{max}}\\mathbb{E}\\left[\\text{return}|\\beta, \\alpha(s)\\right]\n",
    "$$\n",
    "\n",
    "$$\n",
    "v(\\alpha, s)\n",
    "\\ =\\ \n",
    "R(\\alpha)+\\underset{\\ \\beta\\ \\in\\ \\mathcal{A}_{\\alpha(s)}\\!\\!}{\\text{max}}v\\big(\\beta, \\alpha(s)\\big)\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "526142e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "\n",
    "import random\n",
    "import math\n",
    "import numpy as np\n",
    "from inspect import isfunction\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74896442-e50a-4b35-9aa4-d999c4168467",
   "metadata": {},
   "source": [
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de0b5828",
   "metadata": {},
   "source": [
    "## Classes for various aspects of music theory.\n",
    "The various Python classes we define in this section collect important aspects of music theory relevant to problem of voice leading. Using MIDI standard encoding for instance, every note in the scale can be assigned an integer value between $0$ and $127$. In this way, a solution to any voice leading problem can be encoded completely numerically. However, the reward functions for the sequence of step-by-step actions that constitute a proposed solution to a voice leading problem depend on musical theoretical considerations. We will use the classes we define in the present section in order to evaluation remards for our agent's actions."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd98d1ee",
   "metadata": {},
   "source": [
    "### Classes related to harmony and melody."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "861a6c70",
   "metadata": {},
   "source": [
    "#### Class: `Notes`\n",
    "Parent(s): *none*\n",
    "\n",
    "Constructor arguments: *none*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "f9ddc7d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Notes():\n",
    "    def __init__(self):\n",
    "        \n",
    "        valmod12_to_class = {0: ('C', 'C'),\n",
    "            1: ('C♯', 'D♭'),\n",
    "            2: ('D', 'D'),\n",
    "            3: ('D♯', 'E♭'),\n",
    "            4: ('E', 'E'),\n",
    "            5: ('F', 'F'),\n",
    "            6: ('F♯', 'G♭'),\n",
    "            7: ('G', 'G'),\n",
    "            8: ('G♯', 'A♭'),\n",
    "            9: ('A', 'A'),\n",
    "            10: ('A♯', 'B♭'),\n",
    "            11: ('B', 'B')}\n",
    "        self.valmod12_to_class = valmod12_to_class\n",
    "        \n",
    "        all_note_class_names = []\n",
    "        for key in self.valmod12_to_class:\n",
    "            class_pair = self.valmod12_to_class[key]\n",
    "            all_note_class_names.append(class_pair[0])\n",
    "            all_note_class_names.append(class_pair[1])\n",
    "        self.all_note_class_names = all_note_class_names\n",
    "        \n",
    "        class_to_valmod12 = {}\n",
    "        for key in self.valmod12_to_class:\n",
    "            class_pair = self.valmod12_to_class[key]\n",
    "            for entry in class_pair:\n",
    "                class_to_valmod12.update({entry: key})\n",
    "        self.class_to_valmod12 = class_to_valmod12\n",
    "        \n",
    "        value_to_class = {}\n",
    "        for value in range(128):\n",
    "            valmod12 = value%12\n",
    "            class_pair = self.valmod12_to_class[valmod12]\n",
    "            value_to_class.update({value: class_pair})\n",
    "        self.value_to_class = value_to_class\n",
    "        \n",
    "        note_to_value = {}\n",
    "        for value in self.value_to_class:\n",
    "            class_pair = self.value_to_class[value]\n",
    "            sharp_class = class_pair[0]\n",
    "            flat_class = class_pair[1]\n",
    "            valmod12 = value%12\n",
    "            octave = -1 + int((value - valmod12)/12)\n",
    "            note_to_value.update({sharp_class+'{}'.format(octave): value})\n",
    "            note_to_value.update({flat_class+'{}'.format(octave): value})\n",
    "        self.note_to_value = note_to_value"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b42276ae-9e63-41fd-9fe3-8588f0624919",
   "metadata": {},
   "source": [
    "Testing:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "ffd82563-f694-44bc-b0cb-c8e19ba9b9d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "True\n",
      "True\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "notes = Notes()\n",
    "print(notes.valmod12_to_class[8][0] == 'G♯')\n",
    "print(notes.class_to_valmod12['E♭'] == 3)\n",
    "print(notes.value_to_class[54] == ('F♯', 'G♭'))\n",
    "print(notes.note_to_value['E♭2'] == 39)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "538a25c5",
   "metadata": {},
   "source": [
    "#### Class: `Scales`\n",
    "Parent(s):\n",
    "\n",
    "Constructor arguments: *none*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "eaca2af7",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Scales():\n",
    "    def __init__(self):\n",
    "        \n",
    "        # Construct modern mode degrees, ascending and descending, as attributes:\n",
    "        self.long_step_sequence = [2, 2, 1, 2, 2, 2, 1, 2, 2, 1, 2, 2, 2]\n",
    "        \n",
    "        self.mode_start = {'Ionian': 0,\n",
    "            'Dorian': 1,\n",
    "            'Phrygian': 2,\n",
    "            'Lydian': 3,\n",
    "            'Mixolydian': 4,\n",
    "            'Aeolian': 5,\n",
    "            'Locrian': 6}\n",
    "        \n",
    "        modern_mode_steps = {}\n",
    "        for key, value in self.mode_start.items():\n",
    "            mode = key\n",
    "            start_position = value\n",
    "            current_mode_steps = [self.long_step_sequence[i] for i in range(start_position, start_position+7)]\n",
    "            modern_mode_steps.update({mode: current_mode_steps})\n",
    "        self.modern_mode_steps = modern_mode_steps\n",
    "        \n",
    "        updown_mode_degrees = {}\n",
    "        for key, value in self.modern_mode_steps.items():\n",
    "            mode = key\n",
    "            step_sequence = value\n",
    "            degree_sequence = [0]\n",
    "            for i, step in enumerate(step_sequence):\n",
    "                scale_degree = degree_sequence[i]\n",
    "                new_scale_degree = (scale_degree + step)%12\n",
    "                degree_sequence.append(new_scale_degree)\n",
    "                rev_degree_sequence = degree_sequence[::-1]\n",
    "            updown_mode_degrees.update({mode: {'up': copy.deepcopy(degree_sequence),\n",
    "                                     'down': copy.deepcopy(rev_degree_sequence)}})\n",
    "        \n",
    "        # Construct Major mode degrees, ascending and descending, as attributes:\n",
    "        major_updown = updown_mode_degrees['Ionian']\n",
    "        updown_mode_degrees.update({'Major': copy.deepcopy(major_updown)})\n",
    "\n",
    "        # Construct Natural minor mode degrees, ascending and descending, as attributes:\n",
    "        natural_minor_updown = updown_mode_degrees['Aeolian']\n",
    "        updown_mode_degrees.update({'Natural_minor': copy.deepcopy(natural_minor_updown)})\n",
    "\n",
    "        # Construct Harmonic minor mode degrees, ascending and descending, as attributes:\n",
    "        harmonic_minor_steps = [2, 1, 2, 2, 1, 3, 1]\n",
    "        harmonic_minor_degree_sequence = [0]\n",
    "        for i, step in enumerate(harmonic_minor_steps):\n",
    "            scale_degree = harmonic_minor_degree_sequence[i]\n",
    "            new_scale_degree = (scale_degree + step)%12\n",
    "            harmonic_minor_degree_sequence.append(new_scale_degree)\n",
    "            rev_harmonic_minor_degree_sequence = harmonic_minor_degree_sequence[::-1]\n",
    "        updown_mode_degrees.update({'Harmonic_minor': {'up': copy.deepcopy(harmonic_minor_degree_sequence),\n",
    "                                                     'down': copy.deepcopy(rev_harmonic_minor_degree_sequence)}})\n",
    "\n",
    "        # Construct Melodic minor mode degrees, ascending and descending, as attributes:\n",
    "        melodic_minor_steps_up = [2, 2, 1, 2, 2, 2, 1]\n",
    "        melodic_minor_degrees_up = [0]\n",
    "        for i in range(7):\n",
    "            current_degree = melodic_minor_degrees_up[i]\n",
    "            next_degree = (current_degree + melodic_minor_steps_up[i])%12\n",
    "            melodic_minor_degrees_up.append(next_degree)\n",
    "        melodic_minor_steps_down = [2, 2, 1, 2, 1, 2, 2]\n",
    "        melodic_minor_degrees_down = [0]\n",
    "        for i in range(7):\n",
    "            current_degree = melodic_minor_degrees_down[i]\n",
    "            next_degree = (current_degree - melodic_minor_steps_down[i])%12\n",
    "            melodic_minor_degrees_down.append(next_degree)\n",
    "        updown_mode_degrees.update({'Melodic_minor': {'up': copy.deepcopy(melodic_minor_degrees_up),\n",
    "                                                    'down': copy.deepcopy(melodic_minor_degrees_down)}})\n",
    "\n",
    "        # Combine all ascending and descending mode degrees into attribute dictionary:\n",
    "        self.updown_mode_degrees = updown_mode_degrees\n",
    "\n",
    "        # Collect all modes constructed as list attribute:\n",
    "        mode_list = [key for key in self.updown_mode_degrees]\n",
    "        self.mode_list = mode_list\n",
    " \n",
    "    # Method for querying the ascending/descending mode degree dictionary attribute:\n",
    "    def updown_degrees(self, mode):\n",
    "        assert mode in self.mode_list\n",
    "        output = self.updown_mode_degrees[mode]\n",
    "        return output\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33b425b5",
   "metadata": {},
   "source": [
    "Testing:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "6f8baf31",
   "metadata": {},
   "outputs": [],
   "source": [
    "scales = Scales()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c233e02",
   "metadata": {},
   "source": [
    "#### Class: `Key`\n",
    "Parent(s):\n",
    "\n",
    "Constructor arguments:\n",
    "* *root* = `'C'`, \n",
    "* *mode* = `'Major'`\n",
    "\n",
    "**Important.** The constructor for the `Key` class constructs an instance each of the `Notes` and `Scales` classes as attributes of `Key`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "73ab95c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Key():\n",
    "    def __init__(self,\n",
    "                 root = 'C',\n",
    "                 mode = 'Major'):\n",
    "        \n",
    "        self.notes = Notes()\n",
    "        self.scales = Scales()\n",
    "        \n",
    "        assert root in self.notes.all_note_class_names\n",
    "        assert mode in self.scales.mode_list\n",
    "        \n",
    "        self.root_class = root\n",
    "        self.root_valmod12 = self.notes.class_to_valmod12[self.root_class]\n",
    "        \n",
    "        self.mode = mode\n",
    "        \n",
    "        self.scale_degrees = self.scales.updown_degrees(mode = self.mode)\n",
    "        self.up_degrees = self.scale_degrees['up']\n",
    "        self.triad_degrees = [self.up_degrees[i] for i in [0,2,4]]\n",
    "        self.triad_valsmod12 = [(self.root_valmod12 + degree)%12 for degree in self.triad_degrees]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88ca6f4e",
   "metadata": {},
   "source": [
    "Testing:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "810e6ed2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "key = Key(root = 'E', mode = 'Melodic_minor')\n",
    "print(key.triad_degrees == [0, 4, 7])\n",
    "print(key.triad_valsmod12 == [4, 8, 11])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35dad6e7",
   "metadata": {},
   "source": [
    "## Classes for rewards."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51178122",
   "metadata": {},
   "source": [
    "### Classes for vertical, i.e., harmony-based rewards."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7e5a0d0",
   "metadata": {},
   "source": [
    "#### Class: `ConsonanceScheme`\n",
    "Parent(s): *none*\n",
    "\n",
    "Constructor arguments:\n",
    "* *transform* = `standard_transform`, where *standard_transform* = `lambda x: np.pi + np.log(x)`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "7ac72ea5",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ScaleDegreeScheme():\n",
    "    def __init__(self,\n",
    "                 key = Key()):\n",
    "        \n",
    "        self.key = key\n",
    "        \n",
    "        degree_ranking = [1, 5, 4, 3, 6, 7, 2]\n",
    "        self.degree_ranking = [degree - 1 for degree in degree_ranking]\n",
    "        \n",
    "        self.updown_degrees = key.scales.updown_mode_degrees[self.key.mode]\n",
    "        self.updown_ranking = {'up': [self.updown_degrees['up'][self.degree_ranking[n]] for n in range(7)],\n",
    "                              'down': [self.updown_degrees['down'][-self.degree_ranking[n]%7] for n in range(7)]}\n",
    "        \n",
    "        self.sign_dictionary = {1: 'up', 0: 'up', -1: 'down'}\n",
    "\n",
    "        \n",
    "    def horiz_scale_deg_score(self, chord_0, chord_1):\n",
    "        \n",
    "        assert isinstance(chord_0, list) and isinstance(chord_1, list)\n",
    "        assert len(chord_0) == len(chord_1)\n",
    "        \n",
    "        for i in range(len(chord_0)):\n",
    "            in_note = chord_0[i]\n",
    "            out_note = chord_1[i]\n",
    "            sign = np.sign(chord_1[i] - chord_0[i])\n",
    "            key = self.sign_dictionary[sign]\n",
    "            \n",
    "        \n",
    "        \n",
    "        #output = self.transformed_interval_rewards[interval_pair_class]\n",
    "        \n",
    "        #return output\n",
    "        \n",
    "        pass\n",
    "\n",
    "    \n",
    "    def vert_scale_deg_score(self, chord):\n",
    "        \n",
    "        assert isinstance(chord, list)\n",
    "        \n",
    "        for i in range(1, len(chord)):\n",
    "            interval = chord[i-1 : i]\n",
    "        \n",
    "        #output = self.transformed_interval_rewards[interval_pair_class]\n",
    "        \n",
    "        #return output\n",
    "        \n",
    "        pass"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "361fd7cb",
   "metadata": {},
   "source": [
    "Testing:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "4d7314f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0, 4, 3, 2, 5, 6, 1]\n",
      "{'up': [0, 2, 4, 5, 7, 9, 11, 0], 'down': [0, 11, 9, 7, 5, 4, 2, 0]}\n",
      "{'up': [0, 7, 5, 4, 9, 11, 2], 'down': [0, 7, 5, 4, 9, 11, 2]}\n",
      "\n",
      "[0, 4, 3, 2, 5, 6, 1]\n",
      "{'up': [0, 2, 4, 6, 7, 9, 11, 0], 'down': [0, 11, 9, 7, 6, 4, 2, 0]}\n",
      "{'up': [0, 7, 6, 4, 9, 11, 2], 'down': [0, 7, 6, 4, 9, 11, 2]}\n"
     ]
    }
   ],
   "source": [
    "scale_degree_scheme = ScaleDegreeScheme(key = Key('F', 'Major'))\n",
    "print(scale_degree_scheme.degree_ranking)\n",
    "print(scale_degree_scheme.updown_degrees)\n",
    "print(scale_degree_scheme.updown_ranking)\n",
    "\n",
    "print('')\n",
    "\n",
    "scale_degree_scheme = ScaleDegreeScheme(key = Key('F', 'Lydian'))\n",
    "print(scale_degree_scheme.degree_ranking)\n",
    "print(scale_degree_scheme.updown_degrees)\n",
    "print(scale_degree_scheme.updown_ranking)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ec9227e",
   "metadata": {},
   "source": [
    "### Classes for progress-to-final-interval rewards."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed019712",
   "metadata": {},
   "source": [
    "#### Class: `ProgtoFinScheme`\n",
    "Parent(s): *none*\n",
    "\n",
    "Constructor arguments:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "8cc82ce8",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ProgtoFinScheme():\n",
    "    def __init__(self):\n",
    "        pass\n",
    "        \n",
    "    def prog_to_fin_reward(self,\n",
    "                                 interval_0 = (73, 76),\n",
    "                                 interval_1 = (72, 74),\n",
    "                                 final_interval = (35, 45)):\n",
    "        \n",
    "        centroid_0 = (interval_0[0] + interval_0[1])/2\n",
    "        centroid_1 = (interval_1[0] + interval_1[1])/2\n",
    "        final_centroid = (final_interval[0] + final_interval[1])/2\n",
    "        \n",
    "        needed_change = final_centroid - centroid_0\n",
    "        needed_direction = np.sign(needed_change)\n",
    "        \n",
    "        centroid_change = centroid_1 - centroid_0\n",
    "        actual_direction = np.sign(centroid_change)\n",
    "        signed_change = actual_direction * needed_direction\n",
    "        \n",
    "        scale_signed_change = 5 * signed_change\n",
    "        \n",
    "        return scale_signed_change"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0be524f",
   "metadata": {},
   "source": [
    "Testing:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "4788c9a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "prog_to_fin_scheme = ProgtoFinScheme()\n",
    "print(prog_to_fin_scheme.prog_to_fin_reward() > 0.)\n",
    "print(prog_to_fin_scheme.prog_to_fin_reward(interval_1 = (74, 77)) < 0.)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad5c9d46-0e09-49e7-81c0-f95aed244dda",
   "metadata": {},
   "source": [
    "### Classes for rewards that mix horizontal & vertical aspects."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a24e4c9f-ff04-4608-b018-6207939607c2",
   "metadata": {},
   "source": [
    "#### Class: `NonCrossScheme`\n",
    "Parent(s): *none*\n",
    "\n",
    "Constructor arguments:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "0d2ea02d-f6f1-4dc9-93ae-05d173e11032",
   "metadata": {},
   "outputs": [],
   "source": [
    "class NonCrossScheme():\n",
    "    def __init__(self):\n",
    "        pass\n",
    "        \n",
    "    def non_cross_reward(self,\n",
    "                         interval = (76, 73)):\n",
    "        \n",
    "        if interval[0] >= interval[1]:\n",
    "            crossing_penalty = -1 - (interval[0] - interval[1])\n",
    "        \n",
    "        return crossing_penalty"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36644959-d4d1-4975-b983-82b6f8647802",
   "metadata": {},
   "source": [
    "Testing:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "3bd2ef2c-1d3d-454e-9715-0bbbb2de7fdb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-4"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "non_cross_scheme = NonCrossScheme()\n",
    "non_cross_scheme.non_cross_reward()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f4a1019-b434-432f-a1d6-e4bb07d715bc",
   "metadata": {},
   "source": [
    "#### Class: `NonOverlapScheme`\n",
    "Parent(s): *none*\n",
    "\n",
    "Constructor arguments:\n",
    "\n",
    "**Remark.** See the definition [here](https://en.wikipedia.org/wiki/Voice_crossing#Voice_overlapping). Similar to voice crossing, but more like \"immediate displacement.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9877c9e3-5a90-4f00-8a05-3dc2a999e4a9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "b1cb0ba8-9127-4bf4-9a83-fde17b1f5685",
   "metadata": {},
   "source": [
    "Testing:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b45279f-f5ce-4d71-ba1c-205e6035b5e7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "e63efa0a-2d5a-4bcd-bfdc-d0a1cd91452e",
   "metadata": {},
   "source": [
    "#### Class: `NonParallelScheme`\n",
    "Parent(s): `ConsonanceScheme`\n",
    "\n",
    "Constructor arguments:\n",
    "\n",
    "Constructor arguments:\n",
    "* *transform* = `standard_transform`, where *standard_transform* = `lambda x: np.pi + np.log(x)`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "c7db9ff5-9d5b-4bc5-845a-5139032f9a8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class NonParallelScheme(ConsonanceScheme):\n",
    "    standard_transform = lambda x: np.pi + np.log(x)\n",
    "    \n",
    "    def __init__(self,\n",
    "                 transform = standard_transform):\n",
    "        super().__init__(transform = transform)\n",
    "        \n",
    "#    def consonance_reward(self, pair):\n",
    "#        super().consonance_reward(pair)\n",
    "        \n",
    "    def non_cross_reward(self,\n",
    "                         interval_0 = (58, 61),\n",
    "                         interval_1 = (63, 68)):\n",
    "        \n",
    "        lower_step = interval_1[0] - interval_0[0]\n",
    "        upper_step = interval_1[1] - interval_0[1]\n",
    "        \n",
    "        if lower_step == upper_step:\n",
    "            consonance_penalty = -self.consonance_reward((0, lower_step%12))\n",
    "        else:\n",
    "            consonance_penalty = 0\n",
    "        \n",
    "        return consonance_penalty"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53db8b2f-fa65-469f-97d2-4d6f78fa741f",
   "metadata": {},
   "source": [
    "Testing:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "6f3c8275-0ef0-4231-bc4a-284e7d2066c0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "non_parallel_scheme = NonParallelScheme()\n",
    "non_parallel_scheme.non_cross_reward()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01f37d18-523e-48f6-9e40-d71f0edab83c",
   "metadata": {},
   "source": [
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f21c2e70",
   "metadata": {},
   "source": [
    "## Classes for voice leading states & actions."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3c6ed5f",
   "metadata": {},
   "source": [
    "### Classes for \"first species\" states & actions."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eab5cf9c",
   "metadata": {},
   "source": [
    "#### Class: `Interval`\n",
    "Parent(s): *none*\n",
    "\n",
    "Constructor arguments:\n",
    "* *key* = `default_key`, where *default_key* = `Key(root = 'C', mode = 'Major')`,\n",
    "* *note_pair* = `('G5', 'C6')`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "fd52c2a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Interval():\n",
    "    default_key = Key(root = 'C', mode = 'Major')\n",
    "    \n",
    "    def __init__(self,\n",
    "                 key = default_key,\n",
    "                 note_pair = ('A3', 'C4')):\n",
    "        \n",
    "        assert isinstance(key, Key)\n",
    "        assert isinstance(note_pair, tuple)\n",
    "        assert len(note_pair) == 2\n",
    "        for entry in note_pair:\n",
    "            assert isinstance(entry, str)\n",
    "            assert entry in key.all_note_names\n",
    "            \n",
    "        self.key = key\n",
    "        self.note_pair = note_pair\n",
    "        \n",
    "        note_values = [key.notes_to_values[name] for name in self.note_pair]\n",
    "        self.note_values = tuple(note_values)\n",
    "        self.note_pair_class = [(value-key.root_class_degree)%12 for value in self.note_values]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "388f1e62",
   "metadata": {},
   "source": [
    "Testing:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "e2ccdfbf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(57, 60)\n"
     ]
    }
   ],
   "source": [
    "interval = Interval()\n",
    "print(interval.note_values)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9636169f-2182-4ee2-a19f-b1d00b731ee6",
   "metadata": {},
   "source": [
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7d18b98",
   "metadata": {},
   "source": [
    "## Agent classes corresponding to different voice leading species."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b73e07dc",
   "metadata": {},
   "source": [
    "### \"First species\" action-value neural network."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97f0ecc7-ffb0-42e2-ac89-5d34fadd0bb9",
   "metadata": {},
   "source": [
    "#### Function: `randinterval`\n",
    "Arguments: *none*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "1d71dbce-b106-4710-aa4e-708cf87eddcc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def randinterval():\n",
    "    lower_note_value = random.randrange(57, 81)\n",
    "    note_difference = random.randrange(1, 11)\n",
    "    while lower_note_value + note_difference > 81:\n",
    "        note_difference = random.randrange(1, 11)\n",
    "    \n",
    "    output_interval = (lower_note_value, lower_note_value + note_difference)\n",
    "    \n",
    "    return output_interval"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1c8c161-d8cb-494c-9b44-a6226a9da862",
   "metadata": {},
   "source": [
    "Testing:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "9216dd2e-9317-4e90-9f01-b78f8c2fb258",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(79, 81)"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "randinterval()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45b00550-aed5-489d-a73c-9ca72c391351",
   "metadata": {},
   "source": [
    "#### Class: `ActionValue_Spec1`\n",
    "Parent(s): `torch.nn.Module`\n",
    "\n",
    "Constructor arguments:\n",
    "* *layer_count* = `6`\n",
    "* *layer_features* = `1000`\n",
    "\n",
    "**Remark: `ReLU` versus `Softmax`.** Because we're implicitly using the *greedy policy*, which, at each state $s$, always selects the action $\\alpha$ that maximizes the action-value $v_{\\text{greed}}(s,\\alpha)$, it might appear that the neural network that approximates $v_{\\text{greed}}(s,\\alpha)$ should use *softmax* activation at its final layer. However, the specific value of $v_{\\text{greed}}(s,\\alpha)$ is also important. This activation function $v_{\\text{greed}}(s,\\alpha)$ is supposed to output the *excpected return* $\\mathbb{E}_{\\text{greed}}[G|\\alpha,\\pi]$, which is a (potentially weighted) sum of all future rewards that the agent will obtain under the greedy policy. Because we've already specified our rewards implicitly in the various reward functions we defined above, we will run into trouble if we use softmax. Indeed, $0\\le \\text{softmax}(x)\\le 1$, whereas our reward functions can tske all sorts of integer values, sometimes negative. Thus is makes more sense to use `ReLU` or `LeakyReLU` for activation in our neural network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "8f47210f-d07a-461d-a4a8-6723203be546",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ActionValue_Spec1(nn.Module):\n",
    "    def __init__(self, layer_count = 8, layer_features = 1000):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.layer_count = layer_count\n",
    "        self.layer_features = layer_features\n",
    "        \n",
    "        self.layers = nn.ModuleList()\n",
    "        \n",
    "        self.layers.append(nn.Linear(in_features = 3 * 195, out_features = self.layer_features))\n",
    "        for k in range(self.layer_count-2):\n",
    "            self.layers.append(nn.Linear(in_features = self.layer_features, out_features = self.layer_features))\n",
    "        self.layers.append(nn.Linear(in_features = self.layer_features, out_features = 1))\n",
    "        \n",
    "        self.activation = nn.LeakyReLU(negative_slope=0.01)\n",
    "        \n",
    "        possible_pair_indices = {}\n",
    "        pair_counter = 0\n",
    "        for i in range(57,81):\n",
    "            for j in range(1,11):\n",
    "                if i + j < 82:\n",
    "                    possible_pair_indices.update({(i, i+j): copy.deepcopy(pair_counter)})\n",
    "                    pair_counter += 1\n",
    "        self.possible_pair_indices = possible_pair_indices\n",
    "        print('\\nNumber of pairs:', len(self.possible_pair_indices))\n",
    "        \n",
    "        indices_to_pairs = {}\n",
    "        for pair in self.possible_pair_indices:\n",
    "            index = self.possible_pair_indices[pair]\n",
    "            indices_to_pairs.update({index: copy.deepcopy(pair)})\n",
    "        self.indices_to_pairs = indices_to_pairs\n",
    "        \n",
    "    def forward(self, x):\n",
    "        activated_features = x\n",
    "        for layer in self.layers:\n",
    "            output_features = layer(activated_features)\n",
    "            activated_features = self.activation(output_features)\n",
    "            \n",
    "        return activated_features\n",
    "    \n",
    "    \n",
    "    def max_arg(self, pair_0, final_pair):\n",
    "        k = self.possible_pair_indices[pair_0]\n",
    "        s = torch.tensor([float(i == k) for i in range(195)])\n",
    "        \n",
    "        n = self.possible_pair_indices[final_pair]\n",
    "        f = torch.tensor([float(i == n) for i in range(195)])\n",
    "        \n",
    "        entry_0, entry_1 = pair_0[0], pair_0[1]\n",
    "\n",
    "        possible_actions = []\n",
    "        for lower_entry in range(57, 81):\n",
    "            for upper_diff in range(1, 11):\n",
    "                if lower_entry + upper_diff < 82:\n",
    "                    possible_actions.append((lower_entry, lower_entry + upper_diff))\n",
    "\n",
    "        max_val = -math.inf\n",
    "        for pair in possible_actions:\n",
    "            index = self.possible_pair_indices[pair]\n",
    "            a = torch.tensor([float(j == index) for j in range(195)])\n",
    "            model_input = torch.cat((s, a, f))\n",
    "            model_output = self.forward(model_input)\n",
    "            if model_output.item() > max_val:\n",
    "                max_val = model_output.item()\n",
    "                argument_that_maximizes = index\n",
    "        return argument_that_maximizes\n",
    "    \n",
    "    \n",
    "    def epsilon_greedy(self, epsilon, pair_0, final_pair):\n",
    "        greedy_or_random = np.random.choice(['greedy', 'random'], p=[1-epsilon, epsilon])\n",
    "        \n",
    "        if greedy_or_random == 'greedy':\n",
    "            state_index = self.max_arg(pair_0, final_pair)\n",
    "        elif greedy_or_random == 'random':\n",
    "            state_index = random.randrange(0, 195)\n",
    "        \n",
    "        return state_index \n",
    "    \n",
    "    \n",
    "    \n",
    "    def double_max_arg(self, pair_0, final_pair):\n",
    "        \n",
    "        max_arg_1 = self.max_arg(pair_0, final_pair)\n",
    "        pair_1 = self.indices_to_pairs[max_arg_1]\n",
    "        \n",
    "        max_arg_2 = self.max_arg(pair_1, final_pair)\n",
    "        pair_2 = self.indices_to_pairs[max_arg_2]\n",
    "        \n",
    "        return pair_1, pair_2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88a29e88-6e20-49bb-96a0-9c6f975de891",
   "metadata": {},
   "source": [
    "Testing:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "dd404a2f-2c6d-4aa4-a6f7-801566cf7dc1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Number of pairs: 195\n",
      "\n",
      "Initial state: (58, 61)\n",
      "\n",
      "Index of state after action with highest expected return: 184\n",
      "State after action with highest expected return: (76, 81)\n",
      "Value at this state: -4.5131360820960253e-05 \n",
      "\n",
      "\n",
      " ((76, 81), (61, 65))\n"
     ]
    }
   ],
   "source": [
    "action_value_spec1 = ActionValue_Spec1()\n",
    "\n",
    "pair_0 = randinterval()\n",
    "\n",
    "final_pair = randinterval()\n",
    "\n",
    "print('\\nInitial state:', pair_0)\n",
    "\n",
    "epsilon = 0.2\n",
    "\n",
    "print('\\nIndex of state after action with highest expected return:', action_value_spec1.epsilon_greedy(epsilon, pair_0, final_pair))\n",
    "\n",
    "maximizing_argument = action_value_spec1.indices_to_pairs[action_value_spec1.epsilon_greedy(epsilon, pair_0, final_pair)]\n",
    "print('State after action with highest expected return:', maximizing_argument)\n",
    "\n",
    "k = action_value_spec1.possible_pair_indices[pair_0]\n",
    "s = torch.tensor([float(i == k) for i in range(195)])\n",
    "      \n",
    "n = action_value_spec1.possible_pair_indices[final_pair]\n",
    "f = torch.tensor([float(i == n) for i in range(195)])\n",
    "        \n",
    "index = action_value_spec1.possible_pair_indices[maximizing_argument]\n",
    "a = torch.tensor([float(j == index) for j in range(195)])\n",
    "\n",
    "model_input = torch.cat((s, a, f))\n",
    "print('Value at this state:', action_value_spec1.forward(model_input).item(),'\\n')\n",
    "\n",
    "print('\\n', action_value_spec1.double_max_arg(pair_0, final_pair))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9578384d-6719-4610-881e-af68200dd0d1",
   "metadata": {},
   "source": [
    "### \"First species\" voice leading reinforcement learning agent."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5806ac6b",
   "metadata": {},
   "source": [
    "#### Class: `Agent_FirstSpecies`\n",
    "Parent(s): *none*\n",
    "\n",
    "Constructor arguments:\n",
    "* *key* = `default_key`, where *default_key* = `Key(root = 'C', mode = 'Major')`\n",
    "* *start_interval* = `default_start_interval`, where *default_start_interval* = `['C♯7', 'E7']`\n",
    "* *end_interval* = `default_end_interval`, where *default_end_interval* = `['C♯5', 'G5']`\n",
    "* *expected_return* = `nn.Module()`\n",
    "\n",
    "**Remark: Tip for next iteration.** I caused myself a bit of a headache by flip-flopping between inheritance and composition as I built all the classes above. In the nest iteration, I think it would make more sense to package all the underlying music theory classes by using them as constructors in one music theory \"super class,\" and to package all the reward schemes together by using them as arguments in the constructor for a rewards super class. Then use these music theory and rewards super classes as arguments in the constructor for the learning agent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "a22eba0d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Number of pairs: 195\n"
     ]
    }
   ],
   "source": [
    "class Agent_FirstSpecies():\n",
    "    default_key = Key(root = 'C', mode = 'Major')\n",
    "    default_start_interval = ('C♯5', 'E5')\n",
    "    default_end_interval = ('C♯4', 'G4')\n",
    "    \n",
    "    default_accent_pattern = AccentPattern()\n",
    "    \n",
    "    default_actionvalue = ActionValue_Spec1()\n",
    "    \n",
    "    def __init__(self,\n",
    "                 key = default_key,\n",
    "                 start_interval = default_start_interval,\n",
    "                 end_interval = default_end_interval,\n",
    "                 expected_return = default_actionvalue,\n",
    "                 accent_pattern = default_accent_pattern):\n",
    "        \n",
    "        super().__init__()\n",
    "        \n",
    "        assert isinstance(key, Key)\n",
    "        assert isinstance(start_interval, tuple) and isinstance(end_interval, tuple)\n",
    "        assert len(start_interval) == len(end_interval) == 2\n",
    "        for entry in start_interval:\n",
    "            assert entry in key.all_note_names\n",
    "        for entry in end_interval:\n",
    "            assert entry in key.all_note_names\n",
    "            \n",
    "        assert isinstance(expected_return, nn.Module)\n",
    "        \n",
    "        self.key = key\n",
    "        self.start_interval = Interval(key = self.key, note_pair = start_interval)\n",
    "        self.end_interval = Interval(key = self.key, note_pair = end_interval)\n",
    "        \n",
    "        self.end_interval_value = self.end_interval.note_values\n",
    "        self.start_interval_value = self.start_interval.note_values\n",
    "        \n",
    "        self.expected_return = expected_return\n",
    "        \n",
    "        self.value_episode = [self.start_interval_value]\n",
    "        \n",
    "        possible_pair_indices = {}\n",
    "        pair_counter = 0\n",
    "        for i in range(57,81):\n",
    "            for j in range(1,11):\n",
    "                if i + j < 82:\n",
    "                    possible_pair_indices.update({(i, i+j): copy.deepcopy(pair_counter)})\n",
    "                    pair_counter += 1\n",
    "        self.possible_pair_indices = possible_pair_indices\n",
    "        \n",
    "        indices_to_pairs = {}\n",
    "        for pair in self.possible_pair_indices:\n",
    "            index = self.possible_pair_indices[pair]\n",
    "            indices_to_pairs.update({index: copy.deepcopy(pair)})\n",
    "        self.indices_to_pairs = indices_to_pairs\n",
    "        self.number_of_pairs = len(self.indices_to_pairs)\n",
    "        \n",
    "        final_index = self.possible_pair_indices[self.end_interval_value]\n",
    "        end_vector = [float(i == final_index) for i in range(self.number_of_pairs)]\n",
    "        end_tensor = torch.Tensor(end_vector)\n",
    "        self.end_tensor = end_tensor\n",
    "        \n",
    "        tensor_episode = []\n",
    "        pair_index = self.possible_pair_indices[self.start_interval.note_values]\n",
    "        vector = [float(i == pair_index) for i in range(self.number_of_pairs)]\n",
    "        tensor = torch.Tensor(vector)\n",
    "        tensor_episode.append(copy.deepcopy(tensor))\n",
    "        self.tensor_episode = tensor_episode\n",
    "        \n",
    "        self.accent_pattern = accent_pattern\n",
    "        \n",
    "        self.consonance_scheme = ConsonanceScheme()\n",
    "        self.zero_step_scheme = ZeroStepScheme(self.key.root_class_name,\n",
    "                                               self.key.mode)\n",
    "        self.prog_to_fin_scheme = ProgtoFinScheme()\n",
    "        self.step_scheme = StepScheme()\n",
    "        self.triple_step_scheme = TripleStepScheme()\n",
    "        \n",
    "        \n",
    "    def next_interval(self):\n",
    "        last_pair = self.value_episode[-1]\n",
    "        local_arg_max = copy.deepcopy(self.indices_to_pairs[self.expected_return.epsilon_greedy(epsilon, last_pair, self.end_interval_value)])\n",
    "        \n",
    "        self.value_episode.append(local_arg_max)\n",
    "        \n",
    "        new_index = self.possible_pair_indices[local_arg_max]\n",
    "        current_vector = [float(i == new_index) for i in range(self.number_of_pairs)]\n",
    "        current_tensor = torch.Tensor(copy.deepcopy(current_vector))\n",
    "        self.tensor_episode.append(copy.deepcopy(current_tensor))\n",
    "    \n",
    "\n",
    "    def last_reward(self):\n",
    "        \n",
    "        running_reward = 0\n",
    "        \n",
    "        \n",
    "        # ConsonanceScheme rewards:\n",
    "        last_pair = self.value_episode[-1]\n",
    "        last_pair_class = tuple([value%12 for value in last_pair])\n",
    "        \n",
    "        consonance_reward = self.consonance_scheme.consonance_reward(last_pair_class)\n",
    "        running_reward += consonance_reward\n",
    "        \n",
    "        \n",
    "        # ZeroStepScheme rewards:\n",
    "        \n",
    "        # Scale degree reward:\n",
    "        if len(self.value_episode) >= 2:\n",
    "            interval_A = self.value_episode[-2]\n",
    "            interval_B = self.value_episode[-1]\n",
    "            scale_degree_reward = self.zero_step_scheme.scale_degree_reward(interval_A, interval_B)\n",
    "            \n",
    "            running_reward += scale_degree_reward\n",
    "        \n",
    "        # tonic_accents_reward:\n",
    "        beat_in_cycle_number = (len(self.value_episode) - 1)%(self.accent_pattern.total_beat_count)\n",
    "        bar_number = int(np.floor(beat_in_cycle_number/self.accent_pattern.beats_per_measure))-1\n",
    "        beat_in_measure_number = beat_in_cycle_number%(self.accent_pattern.beats_per_measure)\n",
    "        \n",
    "        accents_reward = self.zero_step_scheme.tonic_accents_reward(last_pair,\n",
    "                                                                    bar_number = bar_number,\n",
    "                                                                    beat_number = beat_in_measure_number,\n",
    "                                                                    accent_pattern = self.accent_pattern)\n",
    "        running_reward += accents_reward\n",
    "        \n",
    "        # extrema_triad_reward (one for cycle-wide extrema and one for measure-wide extrema):\n",
    "        where_at = len(self.value_episode)\n",
    "        measure_length = self.accent_pattern.beats_per_measure\n",
    "        if where_at%measure_length == 0:\n",
    "            last_complete_measure = self.value_episode[-(measure_length+1):-1]\n",
    "            extrema_in_measure_reward = self.zero_step_scheme.extrema_triad_reward(interval_sequence = last_complete_measure)\n",
    "            running_reward += extrema_in_measure_reward\n",
    "        \n",
    "        cycle_length = self.accent_pattern.total_beat_count\n",
    "        if where_at%cycle_length == 0:\n",
    "            last_complete_cycle = self.value_episode[-(cycle_length+1):-1]\n",
    "            extrema_in_cycle_reward = self.zero_step_scheme.extrema_triad_reward(interval_sequence = last_complete_cycle)\n",
    "            running_reward += extrema_in_cycle_reward\n",
    "        \n",
    "        \n",
    "        # StepScheme rewards:\n",
    "        if len(self.value_episode) >= 2:\n",
    "            interval_A = self.value_episode[-2]\n",
    "            interval_B = self.value_episode[-1]\n",
    "            step_reward = self.step_scheme.step_reward(interval_A,\n",
    "                                           interval_B)\n",
    "            running_reward += step_reward\n",
    "        \n",
    "        \n",
    "        # TripleStepScheme rewards:\n",
    "\n",
    "        if len(self.value_episode) >= 3:\n",
    "            interval_A = self.value_episode[-3]\n",
    "            interval_B = self.value_episode[-2]\n",
    "            interval_C = self.value_episode[-1]\n",
    "            within_octave_reward = self.triple_step_scheme.within_octave_reward(interval_sequence = [interval_A, interval_B, interval_C])\n",
    "            running_reward += within_octave_reward\n",
    "        \n",
    "        \n",
    "        # ProgtoFinScheme rewards:\n",
    "        prog_to_fin_scalar = 2.0\n",
    "        \n",
    "        if len(self.value_episode) >= 2:\n",
    "            interval_A = self.value_episode[-2]\n",
    "            interval_B = self.value_episode[-1]\n",
    "            prog_to_fin_reward = self.prog_to_fin_scheme.prog_to_fin_reward(interval_0 = interval_A,\n",
    "                                                                            interval_1 = interval_B,\n",
    "                                                                            final_interval = self.end_interval_value)\n",
    "            running_reward += prog_to_fin_scalar * prog_to_fin_reward\n",
    "        \n",
    "        \n",
    "        # No-sticking reward:\n",
    "        if len(self.value_episode) >= 2:\n",
    "            interval_A = self.value_episode[-2]\n",
    "            interval_B = self.value_episode[-1]\n",
    "            \n",
    "            if interval_A == interval_B:\n",
    "                running_reward += -20\n",
    "        \n",
    "        return running_reward"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa9c5c1b",
   "metadata": {},
   "source": [
    "Testing:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "27c0bdbd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " [(73, 76)]\n",
      "[tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])]\n",
      "\n",
      "Number of pairs: 195\n",
      "\n",
      "Initial state: (73, 76)\n",
      "\n",
      " [(73, 76), (72, 74)]\n",
      "\n",
      " 195\n",
      "\n",
      " [(73, 76), (72, 74), (68, 69)]\n",
      "\n",
      " 195\n",
      "\n",
      " [(73, 76), (72, 74), (68, 69), (68, 69)]\n",
      "\n",
      " 195\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "-17.566457547512417"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "agent = Agent_FirstSpecies()\n",
    "print('\\n', agent.value_episode)\n",
    "print(agent.tensor_episode)\n",
    "\n",
    "epsilon = 0.2\n",
    "\n",
    "action_value_spec1 = ActionValue_Spec1()\n",
    "\n",
    "pair_0 = agent.value_episode[0]\n",
    "\n",
    "print('\\nInitial state:', pair_0)\n",
    "\n",
    "agent.next_interval()\n",
    "print('\\n', agent.value_episode)\n",
    "print('\\n', len(agent.tensor_episode[-1]))\n",
    "\n",
    "agent.next_interval()\n",
    "print('\\n', agent.value_episode)\n",
    "print('\\n', len(agent.tensor_episode[-1]))\n",
    "\n",
    "agent.next_interval()\n",
    "print('\\n', agent.value_episode)\n",
    "print('\\n', len(agent.tensor_episode[-1]))\n",
    "\n",
    "agent.last_reward()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ca52481-dccd-4fb3-8b30-1bd0b3cf638f",
   "metadata": {},
   "source": [
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68f2de56-540e-43a8-b58a-29923190cc58",
   "metadata": {},
   "source": [
    "## Training loop(s)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37554400-e971-4bc3-a891-7d52b938d52d",
   "metadata": {},
   "source": [
    "### Attempt 1.\n",
    "#### Verdict(s): \n",
    "* *Loss function unstable.*\n",
    "### Improvement hypotheses, ordered by ease-of-investigation:\n",
    "1. I am back-propping wrong, since I take gradient of *both* expected returns in $\\big(R+v_{\\mathbf{w}}(s_2,s_1)-v_{\\mathbf{w}}(s_1,s_0)\\big)^2$. Try taking gradient only with respect to the term $v_{\\mathbf{w}}(s_1,s_0)$ under the square. ***FIXED***\n",
    "2. I am using the same value for `start_interval` and `end_interval` every episode. This doesn't provide a very representative example of the dataset. Try using a different `start_interval` and `end_interval` every episode. ***FIXED***\n",
    "3. I am updating my weights $\\mathbf{w}$ after running a whole episode. This seems to be a form of *off-policy* learning, which is one third of the [\"deadly triad\"](https://arxiv.org/pdf/1812.02648.pdf). I'm already using another third of the deadly triad: *function approximation*. I don't fully understand what characterizes the last third of the deadly triad — *bootstrapping* — so I can't decide if I'm doing it in this training loop. Anyway, one way to get more \"on-policy\" for the training loop below would be to run backprop after each step of the episode, instead of the whole episode. ***FIXED***\n",
    "4. It seems like the agent is taking large steps. Check scales & signs of reward functions. ***FIXED***\n",
    "5. The agent seems to often have trouble moving in the direction of its final interval. It occurs to me that this is coming from an unintended inductive bias in my model. The action-value function, a.k.a. expected return, only depends on the current state and next state. But the agent should be making different choices when it has a different ending interval, so the model needs to have *ending interval* (and eventually *key*) as that are fixed by choice of agent.\n",
    "6. Because the *beat count* is not a feature of each state of the model, the action-value function can't make use of it. But the model should probably behave differntly went it's near the beginning of a song versus the end of a song.\n",
    "7. **Important!** Instead of using the *greedy policy*, used the $\\epsilon$-*greedy policy* (see p. 100 of [[*Sutton & Barto*]](http://www.incompleteideas.net/book/the-book-2nd.html)). The underlying hypothesis here is that the agent is getting stuck in short cycles of states because it is not exploring enough. An $\\epsilon$-greedy policy is one way to visit states beyond the greedy policy, while staying primarily commited to the greedy policy.\n",
    "\n",
    "**Remarks.** Some notes about issue that came up as I de-bugged, imporved, and fine-tuned the agent and its action-value model:\n",
    "* I'm noticing that sometimes during training, if the value of the loss function gets close to zero, it then explodes within a couple rounds of backprop. Bringing the learning rate down solves this. I've also tried some gradient clipping, but can't tell if it's really doing anything. Regardless, my real issue with this is that I can't understand why it's happening.\n",
    "* *Interval sequence stabilizes way too soon.*\n",
    "* The role of inductive biases is proving to be really interesting as a work on this agent and its action-value function.\n",
    "\n",
    "**Ideas for future agents.**\n",
    "* *Limit octave range.* Try severely restricting the octave range of each voice, for instance to 2 octaves, i.e., to a range of 24 MIDI values. This restricts the original state space $S$, with cardinality $\\#S= 128^2 = 16384$, to a much smaller state space $S_{\\text{res}}$ with cardinality $\\# S_{\\text{res}} = 24^2 = 576$. This will lead to much faster convergence and training. This strategy is also musically viable, since most instruments — including singing human voices — have a severely restricted normal octave range."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "86cd00f1-b69a-4e3f-8689-f86685221b53",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Number of pairs: 195\n"
     ]
    }
   ],
   "source": [
    "act_val = ActionValue_Spec1()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2f1f456-e732-46ac-a6db-cae1036a8fa9",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Beats per episode: 8\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                       | 1/10000 [00:01<4:13:53,  1.52s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average loss this episode: 143.70694065093994\n",
      "Start interval: (60, 65)\n",
      "End interval: (70, 79)\n",
      "Epsiode as MIDI-value sequence: [(60, 65), (63, 73), (67, 70), (59, 69), (59, 69), (59, 69), (59, 69), (59, 69)]\n",
      "Epsiode as note sequence: [('C4', 'F4'), ('D♯4', 'C♯5'), ('G4', 'A♯4'), ('B3', 'A4'), ('B3', 'A4'), ('B3', 'A4'), ('B3', 'A4'), ('B3', 'A4')]\n",
      "Rewards: [10.983342435227932, 11.507267942683661, -4.909990898105401, -17.055631923746425, -17.055631923746425, -17.055631923746425, -16.055631923746425]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|▎                                    | 101/10000 [02:47<4:47:14,  1.74s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average loss this episode: 81.81544780731201\n",
      "Start interval: (58, 63)\n",
      "End interval: (75, 81)\n",
      "Epsiode as MIDI-value sequence: [(58, 63), (60, 69), (59, 69), (68, 76), (69, 74), (59, 69), (68, 76), (59, 69)]\n",
      "Epsiode as note sequence: [('A♯3', 'D♯4'), ('C4', 'A4'), ('B3', 'A4'), ('G♯4', 'E5'), ('A4', 'D5'), ('B3', 'A4'), ('G♯4', 'E5'), ('B3', 'A4')]\n",
      "Rewards: [13.525520144465276, -6.922298590413093, 13.562073440342685, -5.734797412856095, -6.999949432420371, 12.562073440342685, -5.025713224559435]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  2%|▋                                    | 175/10000 [04:42<4:22:53,  1.61s/it]"
     ]
    }
   ],
   "source": [
    "#notesnotesnotes = Notes()\n",
    "\n",
    "#agent = Agent_FirstSpecies(start_interval = ('C♯5', 'E5'),\n",
    "#                           end_interval = ('C4', 'G4'),\n",
    "#                           expected_return = act_val)\n",
    "\n",
    "episode_count = 10000\n",
    "present_bias = 1.0\n",
    "cycles_per_episode = 1\n",
    "learning_rate = 0.001\n",
    "\n",
    "epsilon = 0.2\n",
    "\n",
    "beats_per_episode = cycles_per_episode * agent.accent_pattern.total_beat_count\n",
    "print('\\nBeats per episode:', beats_per_episode)\n",
    "\n",
    "loss_function = nn.MSELoss().double()\n",
    "\n",
    "# Optimizers specified in the torch.optim package\n",
    "optimizer = torch.optim.SGD(agent.expected_return.parameters(), lr = learning_rate, momentum = 0.9)\n",
    "\n",
    "for episode_number in tqdm(range(episode_count)):\n",
    "    \n",
    "    episode_loss = 0.0\n",
    "    \n",
    "    argument_that_maximizes = 0\n",
    "    \n",
    "    agent.start_interval_value = randinterval()\n",
    "    \n",
    "    agent.end_interval_value = randinterval()\n",
    "    final_index = agent.possible_pair_indices[agent.end_interval_value]\n",
    "    end_vector = [float(i == final_index) for i in range(agent.number_of_pairs)]\n",
    "    end_tensor = torch.Tensor(end_vector)\n",
    "    agent.end_tensor = end_tensor\n",
    "    \n",
    "    agent.value_episode = [agent.start_interval_value]\n",
    "\n",
    "    tensor_episode = []\n",
    "    pair_index = agent.possible_pair_indices[agent.start_interval_value]\n",
    "    vector = [float(i == pair_index) for i in range(agent.number_of_pairs)]\n",
    "    tensor = torch.Tensor(vector)\n",
    "    tensor_episode.append(copy.deepcopy(tensor))\n",
    "    agent.tensor_episode = tensor_episode\n",
    "\n",
    "#    print('\\nEpisode number: {} out of {}'.format(episode_number+1, episode_count))\n",
    "    \n",
    "    reward_list = []\n",
    "    note_list = []\n",
    "    note_list.append((notesnotesnotes.values_to_notes[agent.start_interval_value[0]][0],\n",
    "                      notesnotesnotes.values_to_notes[agent.start_interval_value[1]][0]))\n",
    "    \n",
    "    # Episode-generating loop:\n",
    "    for beat_counter in range(beats_per_episode):\n",
    "        \n",
    "        agent.next_interval()\n",
    "        reward = copy.deepcopy(agent.last_reward())\n",
    "        reward_list.append(reward)\n",
    "        \n",
    "#        print('State {}:'.format(beat_counter), agent.value_episode[beat_counter])\n",
    "    \n",
    "    # # Expected-return-adjusting loop:\n",
    "    # for beat_number in range(beats_per_episode):\n",
    "\n",
    "        # Zero your gradients for every batch, i.e., at every beat!\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        \n",
    "        if 1 < beat_counter < beats_per_episode - 1:\n",
    "            \n",
    "            beat_number = beat_counter - 2\n",
    "        \n",
    "            previous_state = agent.value_episode[beat_number]\n",
    "            previous_tensor = agent.tensor_episode[beat_number]\n",
    "        \n",
    "            current_state = agent.value_episode[beat_number + 1]\n",
    "            current_tensor = agent.tensor_episode[beat_number + 1]\n",
    "            \n",
    "            action_1 = torch.cat((previous_tensor, current_tensor, agent.end_tensor))\n",
    "            return_at_current_state = agent.expected_return.forward(action_1)\n",
    "            \n",
    "            current_reward = reward_list[beat_number]\n",
    "            tensor_reward = torch.tensor([current_reward]).float()\n",
    "            \n",
    "            next_state = agent.value_episode[beat_number + 2]\n",
    "            next_tensor = agent.tensor_episode[beat_number + 2]\n",
    "            \n",
    "            action_2 = torch.cat((current_tensor, next_tensor, agent.end_tensor))\n",
    "            return_at_next_state = torch.tensor([agent.expected_return.forward(action_2).item()])\n",
    "            #print(return_at_next_state)\n",
    "            \n",
    "            # print(current_state[0])\n",
    "            # print(current_state[1])\n",
    "            # print(notesnotesnotes.notes_to_values)\n",
    "            note_list.append((notesnotesnotes.values_to_notes[current_state[0]][0],\n",
    "                              notesnotesnotes.values_to_notes[current_state[1]][0]))\n",
    "            \n",
    "            # Compute the loss and its gradients\n",
    "            loss = loss_function(tensor_reward + present_bias * return_at_next_state, return_at_current_state)\n",
    "#           \n",
    "            episode_loss += loss.item()\n",
    "            \n",
    "            loss.backward()\n",
    "\n",
    "            # Adjust learning weights\n",
    "            optimizer.step()\n",
    "            \n",
    "        \n",
    "        elif beat_counter == beats_per_episode - 1:\n",
    "            \n",
    "            beat_number = beat_counter - 2\n",
    "        \n",
    "            previous_state = agent.value_episode[beat_number]\n",
    "            previous_tensor = agent.tensor_episode[beat_number]\n",
    "        \n",
    "            current_state = agent.value_episode[beat_number + 1]\n",
    "            current_tensor = agent.tensor_episode[beat_number + 1]\n",
    "            \n",
    "            action_1 = torch.cat((previous_tensor, current_tensor, agent.end_tensor))\n",
    "            return_at_current_state = agent.expected_return.forward(action_1)\n",
    "            \n",
    "            current_reward = reward_list[beat_number]\n",
    "            tensor_reward = torch.tensor([current_reward]).float()\n",
    "            \n",
    "            next_state = agent.value_episode[beat_number + 2]\n",
    "        \n",
    "            note_list.append((notesnotesnotes.values_to_notes[current_state[0]][0],\n",
    "                              notesnotesnotes.values_to_notes[current_state[1]][0]))\n",
    "        \n",
    "            note_list.append((notesnotesnotes.values_to_notes[next_state[0]][0],\n",
    "                              notesnotesnotes.values_to_notes[next_state[1]][0]))\n",
    "            \n",
    "            # Compute the loss and its gradients\n",
    "            loss = loss_function(tensor_reward, return_at_current_state)\n",
    "\n",
    "            episode_loss += loss.item()\n",
    "\n",
    "            loss.backward()\n",
    "\n",
    "            # Adjust learning weights\n",
    "            optimizer.step()\n",
    "            \n",
    "#            print('Last step completed')\n",
    "        \n",
    "        nn.utils.clip_grad_norm_(parameters = agent.expected_return.parameters(), max_norm=10, norm_type=2.0)\n",
    "        \n",
    "     \n",
    "    if episode_number%100 == 0:\n",
    "        print('Average loss this episode:', episode_loss/8)\n",
    "        print('Start interval:', agent.start_interval_value)\n",
    "        print('End interval:', agent.end_interval_value)\n",
    "        print('Epsiode as MIDI-value sequence:', agent.value_episode[0:8])\n",
    "        print('Epsiode as note sequence:', note_list)\n",
    "        print('Rewards:', reward_list[0:-1])\n",
    "    #print('\\n', reward_list)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c97e8ad6-47e0-45f2-8a95-6a639940804d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2345ae18-1a9e-4201-89bc-c946e430241f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "toc-autonumbering": true,
  "vscode": {
   "interpreter": {
    "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
